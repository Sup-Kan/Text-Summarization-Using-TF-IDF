import requests
from bs4 import BeautifulSoup
import re
import time
from src.crawling.config import HEADERS, REQUEST_TIMEOUT, URL, MAX_RETRIES, RETRY_DELAY
from src.crawling.config import DELAY_BETWEEN_REQUESTS, DELAY_BETWEEN_SUBCATEGORIES
from src.crawling.config import EXCLUDED_CATEGORIES
from src.crawling.utils import normalize_url, print_progress, retry, get_logger
from src.crawling.utils import extract_date_from_text

class NewsScraper:
    """Class ch√≠nh ƒë·ªÉ crawl tin t·ª©c."""

    def __init__(self, storage):
        self.storage = storage
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
        self.logger = get_logger(__name__)
        self.logger.info("NewsScraper initialized")

    @retry(max_attempts=MAX_RETRIES, delay=RETRY_DELAY, exceptions=(requests.RequestException,))
    def _make_request(self, url):
        """Th·ª±c hi·ªán HTTP request v·ªõi retry."""
        try:
            self.logger.debug(f"Requesting URL: {url}")
            response = self.session.get(url, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            self.logger.debug(f"‚úì Request successful: {url} (status={response.status_code})")
            return BeautifulSoup(response.content, 'html.parser')
        except requests.RequestException as e:
            self.logger.error(f"‚úó Request failed for {url}: {e}")
            raise

    def get_main_categories(self):
        """L·∫•y danh s√°ch c√°c chuy√™n m·ª•c ch√≠nh."""
        print_progress("ƒêang l·∫•y danh s√°ch chuy√™n m·ª•c ch√≠nh...")
        self.logger.info("Fetching main categories from homepage")

        soup = self._make_request(URL)
        if not soup:
            self.logger.warning("No soup returned from homepage")
            return []

        main_menu = soup.find('div', class_='header__menu')
        if not main_menu:
            self.logger.warning("Main menu not found on homepage")
            return []

        menu_items = main_menu.find('ul').find_all('li', recursive=False)
        categories = []

        for li in menu_items:
            anchor_tag = li.find('a', class_='nav-link') or li.find('a')

            if anchor_tag:
                title = anchor_tag.get_text(strip=True)
                link = anchor_tag.get('href')

                if title and link and title.lower() not in EXCLUDED_CATEGORIES:
                    full_link = normalize_url(link, URL)
                    categories.append({
                        'title': title,
                        'link': full_link
                    })
                    self.logger.info(f"Found category: {title} ({full_link})")

        self.logger.info(f"Total categories found: {len(categories)}")
        print_progress(f"‚úì T√¨m th·∫•y {len(categories)} chuy√™n m·ª•c ch√≠nh", level=1)
        return categories

    def get_subcategories_from_category(self, category_url):
        """L·∫•y c√°c chuy√™n m·ª•c con t·ª´ breadcrumb."""
        print_progress(f"‚Üí ƒêang l·∫•y chuy√™n m·ª•c con...", level=1)

        soup = self._make_request(category_url)
        if not soup:
            return []

        subcategories = []
        breadcrumb = soup.find('div', class_='list__breadcrumb')
        
        if breadcrumb:
            breadcrumb_ul = breadcrumb.find('ul')
            if breadcrumb_ul:
                breadcrumb_items = breadcrumb_ul.find_all('li')

                for li in breadcrumb_items:
                    h1_tag = li.find('h1')
                    sub_anchor = h1_tag.find('a') if h1_tag else li.find('a')

                    if sub_anchor:
                        sub_title = sub_anchor.get_text(strip=True)
                        sub_link = sub_anchor.get('href')

                        if sub_title and sub_link and sub_link not in ['/', category_url]:
                            full_link = normalize_url(sub_link, URL)

                            if not any(s['link'] == full_link for s in subcategories):
                                subcategories.append({
                                    'title': sub_title,
                                    'link': full_link
                                })
                                self.logger.info(f"Found subcategory: {sub_title}")

        print_progress(f"‚úì T√¨m th·∫•y {len(subcategories)} chuy√™n m·ª•c con", level=2)
        return subcategories

    def get_articles_from_page(self, page_url, max_articles=10):
        """L·∫•y c√°c b√†i b√°o t·ª´ trang."""
        print_progress(f"‚Üí ƒêang l·∫•y b√†i b√°o...", level=2)

        soup = self._make_request(page_url)
        if not soup:
            return []

        articles = []
        seen_links = set()

        # Strategy 1: Find in h2 and h3
        title_tags = soup.find_all(['h2', 'h3'])
        
        for tag in title_tags:
            link_tag = tag.find('a', href=re.compile(r'\.htm$'))
            if link_tag:
                title = link_tag.get_text(strip=True)
                link = link_tag.get('href')
                
                if title and link:
                    full_link = normalize_url(link, URL)
                    
                    if full_link not in seen_links:
                        seen_links.add(full_link)
                        articles.append({
                            'title': title,
                            'link': full_link
                        })

        # Strategy 2: Find by common classes (fallback)
        if not articles:
            article_links = soup.find_all('a', {
                'href': re.compile(r'\.htm$'),
                'class': re.compile(r'box-category-link-title|title-news|article-title', re.I)
            })
            
            for link_tag in article_links:
                title = link_tag.get_text(strip=True)
                link = link_tag.get('href')
                
                if title and link:
                    full_link = normalize_url(link, URL)
                    
                    if full_link not in seen_links:
                        seen_links.add(full_link)
                        articles.append({
                            'title': title,
                            'link': full_link
                        })

        # Apply limit
        if max_articles:
            articles = articles[:max_articles]

        print_progress(f"‚úì T√¨m th·∫•y {len(articles)} b√†i b√°o", level=3)
        self.logger.info(f"Found {len(articles)} articles on {page_url}")
        return articles

    def _extract_author_from_paragraphs(self, paragraphs):
        """
        Tr√≠ch xu·∫•t t√°c gi·∫£ t·ª´ th·∫ª <p> cu·ªëi c√πng.
        Th∆∞·ªùng l√†: <p><b>T√™n t√°c gi·∫£</b></p>
        
        Returns:
            tuple: (author_name, paragraphs_without_author)
        """
        if not paragraphs:
            return None, paragraphs
        
        # Ki·ªÉm tra th·∫ª <p> cu·ªëi c√πng
        last_p = paragraphs[-1]
        
        # T√¨m th·∫ª <b> trong <p> cu·ªëi
        b_tag = last_p.find('b')
        
        if b_tag:
            author_text = b_tag.get_text(strip=True)
            
            # Validate: kh√¥ng ch·ª©a c√°c t·ª´ kh√≥a kh√¥ng ph·∫£i t√™n
            # (tr√°nh nh·∫ßm v·ªõi in ƒë·∫≠m b√¨nh th∆∞·ªùng)
            invalid_keywords = ['ngu·ªìn', 'theo', 'tham kh·∫£o', 'xem th√™m', 'li√™n h·ªá']
            
            if author_text and len(author_text) < 50:  # T√™n kh√¥ng qu√° d√†i
                is_valid = True
                for keyword in invalid_keywords:
                    if keyword in author_text.lower():
                        is_valid = False
                        break
                
                if is_valid:
                    self.logger.info(f"Extracted author: {author_text}")
                    # Lo·∫°i b·ªè th·∫ª <p> cu·ªëi kh·ªèi danh s√°ch
                    return author_text, paragraphs[:-1]
        
        return None, paragraphs

    def get_article_content(self, article_url, article_title):
        """
        L·∫•y n·ªôi dung ƒë·∫ßy ƒë·ªß + metadata.
        QUAN TR·ªåNG: Tr√≠ch xu·∫•t t√°c gi·∫£ v√† lo·∫°i b·ªè kh·ªèi n·ªôi dung.
        
        Returns:
            tuple: (content, metadata_dict)
        """
        soup = self._make_request(article_url)
        if not soup:
            return None, {}

        # === 1. L·∫•y N·ªòI DUNG CH√çNH ===
        content_div = soup.find('div', class_='detail-content')
        
        if not content_div:
            self.logger.warning(f"No content div found for {article_url}")
            return None, {}
        
        # L·∫•y t·∫•t c·∫£ c√°c th·∫ª p, h2, h3
        paragraphs = content_div.find_all(['p', 'h2', 'h3'])
        
        if not paragraphs:
            self.logger.warning(f"No paragraphs found in content div for {article_url}")
            return None, {}

        # === 2. TR√çCH XU·∫§T T√ÅC GI·∫¢ V√Ä LO·∫†I B·ªé KH·ªéI N·ªòI DUNG ===
        author, cleaned_paragraphs = self._extract_author_from_paragraphs(paragraphs)
        
        # Gh√©p n·ªôi dung (ƒë√£ lo·∫°i b·ªè t√°c gi·∫£)
        content_parts = [p.get_text(strip=True) for p in cleaned_paragraphs if p.get_text(strip=True)]
        main_content = '\n\n'.join(content_parts) if content_parts else ""

        # === 3. TR√çCH XU·∫§T METADATA ===
        metadata = {
            'date': None,
            'author': author,  # ƒê√£ tr√≠ch xu·∫•t ·ªü tr√™n
            'source': 'baochinhphu.vn'
        }

        # T√¨m ng√†y ƒëƒÉng
        detail_time = soup.find('div', class_='detail-time') or soup.find('span', class_='time')
        if detail_time:
            date_text = detail_time.get_text(strip=True)
            metadata['date'] = extract_date_from_text(date_text)
            self.logger.debug(f"Extracted date: {metadata['date']}")

        # Fallback: t√¨m ng√†y trong breadcrumb ho·∫∑c meta tags
        if not metadata['date']:
            meta_date = soup.find('meta', {'property': 'article:published_time'})
            if meta_date:
                metadata['date'] = meta_date.get('content', '')[:10]  # YYYY-MM-DD

        self.logger.info(f"Extracted metadata: author={metadata['author']}, date={metadata['date']}")

        return main_content, metadata

    def crawl_category(self, category, max_subcategories, max_articles):
        """Crawl m·ªôt chuy√™n m·ª•c ho√†n ch·ªânh."""
        stats = {'subcategories': 0, 'articles': 0}

        # 1. Crawl category ch√≠nh
        print_progress("üìÇ Crawl b√†i t·ª´ chuy√™n m·ª•c ch√≠nh...", level=1)
        main_articles = self.get_articles_from_page(category['link'], max_articles)

        for article_idx, article in enumerate(main_articles, 1):
            print_progress(f"üìÑ B√†i {article_idx}/{len(main_articles)}: {article['title'][:50]}...", level=2)

            content, metadata = self.get_article_content(article['link'], article['title'])

            if content:
                success = self.storage.save_article(
                    article_content=content,
                    title=article['title'],
                    url=article['link'],
                    metadata=metadata,
                    category=category['title'],
                    subcategory=None
                )

                if success:
                    stats['articles'] += 1

            time.sleep(DELAY_BETWEEN_REQUESTS)

        # 2. L·∫•y subcategories
        subcategories = self.get_subcategories_from_category(category['link'])
        
        if max_subcategories:
            subcategories = subcategories[:max_subcategories]

        # 3. Crawl subcategories
        for sub_idx, subcategory in enumerate(subcategories, 1):
            print_progress(f"üìÅ Chuy√™n m·ª•c con {sub_idx}/{len(subcategories)}: {subcategory['title']}", level=1)

            sub_articles = self.get_articles_from_page(subcategory['link'], max_articles)

            for article_idx, article in enumerate(sub_articles, 1):
                print_progress(f"üìÑ B√†i {article_idx}/{len(sub_articles)}: {article['title'][:50]}...", level=2)

                content, metadata = self.get_article_content(article['link'], article['title'])

                if content:
                    success = self.storage.save_article(
                        article_content=content,
                        title=article['title'],
                        url=article['link'],
                        metadata=metadata,
                        category=category['title'],
                        subcategory=subcategory['title']
                    )

                    if success:
                        stats['articles'] += 1

                time.sleep(DELAY_BETWEEN_REQUESTS)

            if sub_articles:
                stats['subcategories'] += 1

            time.sleep(DELAY_BETWEEN_SUBCATEGORIES)

        return stats